{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Mistral AI, Azure AI Search and Azure AI Studio\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to integrate Mistral Embeddings with Azure AI Search as a vector store, and use the results to ground responses in the Mistral Chat Completion Model.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Mistral AI API Key OR Azure AI Studio Deployed Mistral Chat Completion Model and Azure AI Studio API Key\n",
    "- Azure AI Search service\n",
    "- Python 3.x environment with necessary libraries installed\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Install required packages\n",
    "2. Load data and generate Mistral embeddings\n",
    "3. Index embeddings in Azure AI Search\n",
    "4. Perform search using Azure AI Search\n",
    "5. Ground search results in Mistral Chat Completion Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Packages\n",
    "!pip install azure-search-documents==11.6.0b4\n",
    "!pip install azure-identity==1.16.0 datasets==2.19.1 mistralai==0.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Generate Mistral Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'content', 'prechunk_id', 'postchunk_id', 'arxiv_id', 'references'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\n",
    "    \"jamescalam/ai-arxiv2-semantic-chunks\",\n",
    "    split=\"train[:10000]\"\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 10K chunks, where each chunk is roughly the length of 1-2 paragraphs in length. Here is an example of a single record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2401.04088#0',\n",
       " 'title': 'Mixtral of Experts',\n",
       " 'content': '4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.',\n",
       " 'prechunk_id': '',\n",
       " 'postchunk_id': '2401.04088#1',\n",
       " 'arxiv_id': '2401.04088',\n",
       " 'references': ['1905.07830']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the data into the format we need, this will contain `id`, `title`, `content` (which we will embed), and `arxiv_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'content', 'arxiv_id'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.remove_columns([\"prechunk_id\", \"postchunk_id\", \"references\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define an embedding model to create our embedding vectors for retrieval, for that we will be using Mistral AI's `mistral-embed`. There is some cost associated with this model, so be aware of that (costs for running this notebook are <$1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mistralai.client import MistralClient\n",
    "import getpass  # console.mistral.ai/api-keys/\n",
    "\n",
    "mistral_api_key = os.getenv(\"MISTRAL_API_KEY\") or getpass.getpass(\"Enter your Mistral API key: \")\n",
    "\n",
    "mistral = MistralClient(api_key=mistral_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = \"mistral-embed\"\n",
    "\n",
    "embeds = mistral.embeddings(\n",
    "    model=embed_model, input=[\"this is a test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the dimensionality of our returned embeddings, which we'll need soon when initializing our vector index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims = len(embeds.data[0].embedding)\n",
    "dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Embeddings into Azure AI Search\n",
    "\n",
    "Now we create our vector DB to store our vectors. For this, we need to set up an [Azure AI Search service](https://portal.azure.com/#create/Microsoft.Search).\n",
    "\n",
    "There are two ways to authenticate to Azure AI Search:\n",
    "\n",
    "1. **Service Key**: The service key can be found in the \"Settings -> Keys\" section in the left navbar of the Azure portal dashboard. Make sure to select the ADMIN key.\n",
    "2. **Managed Identity**: Using Microsoft Entra ID (f.k.a. Azure Active Directory) is a more secure and recommended way to authenticate. You can follow the instructions in the [official Microsoft documentation](https://learn.microsoft.com/azure/search/search-security-rbac) to set up Managed Identity.\n",
    "\n",
    "For more detailed instructions on creating an Azure AI Search service, please refer to the [official Microsoft documentation](https://learn.microsoft.com/azure/search/search-create-service-portal).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate into Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AAD for authentication.\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "# Configuration variable\n",
    "USE_AAD_FOR_SEARCH = True\n",
    "\n",
    "SEARCH_SERVICE_ENDPOINT = os.getenv(\"SEARCH_SERVICE_ENDPOINT\") or getpass(\"Enter your Azure AI Search Service Endpoint: \")\n",
    "\n",
    "def authenticate_azure_search(use_aad_for_search=False):\n",
    "    if use_aad_for_search:\n",
    "        print(\"Using AAD for authentication.\")\n",
    "        credential = DefaultAzureCredential()\n",
    "    else:\n",
    "        print(\"Using API keys for authentication.\")\n",
    "        api_key = os.getenv(\"SEARCH_SERVICE_API_KEY\") or getpass(\"Enter your Azure AI Search Service API Key: \")\n",
    "        if api_key is None:\n",
    "            raise ValueError(\"API key must be provided if not using AAD for authentication.\")\n",
    "        credential = AzureKeyCredential(api_key)\n",
    "    return credential\n",
    "\n",
    "azure_search_credential = authenticate_azure_search(\n",
    "    use_aad_for_search=USE_AAD_FOR_SEARCH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ai-arxiv2-semantic-chunks created\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchFieldDataType,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    SearchIndex,\n",
    ")\n",
    "\n",
    "DIMENSIONS = 1024\n",
    "HNSW_PARAMETERS = {\"m\": 4, \"metric\": \"cosine\", \"ef_construction\": 400, \"ef_search\": 500}\n",
    "INDEX_NAME = \"ai-arxiv2-semantic-chunks\"  # replace with your actual index name\n",
    "\n",
    "\n",
    "def create_index(endpoint, credential):\n",
    "    # Create a search index\n",
    "    index_client = SearchIndexClient(endpoint=endpoint, credential=credential)\n",
    "    fields = [\n",
    "        SimpleField(\n",
    "            name=\"id\",\n",
    "            type=SearchFieldDataType.String,\n",
    "            key=True,\n",
    "            sortable=False,\n",
    "            filterable=True,\n",
    "            facetable=False,\n",
    "        ),\n",
    "        SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "        SearchableField(\n",
    "            name=\"arxiv_id\", type=SearchFieldDataType.String, filterable=True\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"embedding\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            vector_search_dimensions=DIMENSIONS,\n",
    "            vector_search_profile_name=\"myHnswProfile\",\n",
    "            hidden=False,  # Set to true if you want to retrieve the embeddings in the search results\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Configure the vector search configuration\n",
    "    vector_search = VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(name=\"myHnsw\", parameters=HNSW_PARAMETERS)\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"myHnswProfile\",\n",
    "                algorithm_configuration_name=\"myHnsw\",\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    semantic_config = SemanticConfiguration(\n",
    "        name=\"my-semantic-config\",\n",
    "        prioritized_fields=SemanticPrioritizedFields(\n",
    "            title_field=SemanticField(field_name=\"title\"),\n",
    "            keywords_fields=[SemanticField(field_name=\"arxiv_id\")],\n",
    "            content_fields=[SemanticField(field_name=\"content\")],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Create the semantic settings with the configuration\n",
    "    semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "    # Create the search index with the semantic settings\n",
    "    index = SearchIndex(\n",
    "        name=INDEX_NAME,\n",
    "        fields=fields,\n",
    "        vector_search=vector_search,\n",
    "        semantic_search=semantic_search,\n",
    "    )\n",
    "    result = index_client.create_or_update_index(index)\n",
    "    print(f\" {result.name} created\")\n",
    "    \n",
    "create_index(SEARCH_SERVICE_ENDPOINT, azure_search_credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate Cost for Embedding Generation\n",
    "\n",
    "As per the information from [Lunary.ai's Mistral Tokenizer](https://lunary.ai/mistral-tokenizer), one token is approximately equivalent to five characters of text. \n",
    "\n",
    "According to [Mistral's Pricing](https://mistral.ai/technology/#pricing), the cost for using `mistral-embed` is $0.1 per 1M tokens for both inputs and outputs.\n",
    "\n",
    "In the following code block, we will calculate the estimated cost for generating embeddings based on the size of our dataset and these pricing details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated cost for generating embeddings: $0.19047898000000002\n"
     ]
    }
   ],
   "source": [
    "def estimate_cost(data, cost_per_million_tokens=0.1):\n",
    "    total_characters = sum(len(entry['content']) for entry in data)\n",
    "    total_tokens = total_characters / 5  # 1 token is approximately 5 characters\n",
    "    total_tokens_in_millions = total_tokens / 1_000_000\n",
    "    total_cost = total_tokens_in_millions * cost_per_million_tokens\n",
    "    return total_cost\n",
    "\n",
    "estimated_cost = estimate_cost(data)\n",
    "print(f\"Estimated cost for generating embeddings: ${estimated_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Dataset for Azure AI Search Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform your dataset into the format required by Azure AI Search\n",
    "def transform_to_search_document(record):\n",
    "    return {\n",
    "        \"id\": record[\"id\"],\n",
    "        \"arxiv_id\": record[\"arxiv_id\"],\n",
    "        \"title\": record[\"title\"],\n",
    "        \"content\": record[\"content\"]\n",
    "    }\n",
    "    \n",
    "# Transform all documents in the dataset\n",
    "transformed_documents = [transform_to_search_document(doc) for doc in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(documents, model, batch_size=20):\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        contents = [doc['content'] for doc in batch]\n",
    "        embeds = mistral.embeddings(model=model, input=contents)\n",
    "        for j, document in enumerate(batch):\n",
    "            document['embedding'] = embeds.data[j].embedding\n",
    "    return documents\n",
    "\n",
    "embed_model = \"mistral-embed\"\n",
    "generate_embeddings(transformed_documents, embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure AI Search doesn't allow certain unsafe keys so we'll base64 encode `id` here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_key(key):\n",
    "    return base64.urlsafe_b64encode(key.encode()).decode()\n",
    "\n",
    "# Use the function to encode keys\n",
    "for document in transformed_documents:\n",
    "    document['id'] = encode_key(document['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 10000 documents in total\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents import SearchIndexingBufferedSender\n",
    "\n",
    "\n",
    "def upload_documents(index_name, endpoint, credential, documents):\n",
    "    buffered_sender = SearchIndexingBufferedSender(endpoint=endpoint, index_name=index_name, credential=credential)\n",
    "\n",
    "    for document in documents:\n",
    "        buffered_sender.upload_documents(documents=[document])\n",
    "\n",
    "    # Flush remaining documents and wait for completion\n",
    "    buffered_sender.flush()\n",
    "\n",
    "    print(f\"Uploaded {len(documents)} documents in total\")\n",
    "\n",
    "# Use the function to upload documents\n",
    "upload_documents(\n",
    "    INDEX_NAME,\n",
    "    SEARCH_SERVICE_ENDPOINT,\n",
    "    azure_search_credential,\n",
    "    transformed_documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SearchClient\n",
    "search_client = SearchClient(\n",
    "    endpoint=SEARCH_SERVICE_ENDPOINT, \n",
    "    index_name=INDEX_NAME, \n",
    "    credential=azure_search_credential\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: TWpRd01TNHdOREE0T0NNNQ==\n",
      "Arxiv ID: 2401.04088\n",
      "Title: Mixtral of Experts\n",
      "Score: 0.7831388\n",
      "Content: Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â = Mistral Â° 20 â e LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params Figure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math. Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks. Size and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories. Note that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization.\n",
      "--------------------------------------------------\n",
      "ID: TWpNeE1DNHdOamd5TlNNeA==\n",
      "Arxiv ID: 2310.06825\n",
      "Title: Mistral 7B\n",
      "Score: 0.7815678\n",
      "Content: Mistral 7B outperforms the previous best 13B model (Llama 2, [26]) across all tested benchmarks, and surpasses the best 34B model (LLaMa 34B, [25]) in mathematics and code generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [20], without sacrificing performance on non-code related benchmarks. Mistral 7B leverages grouped-query attention (GQA) [1], and sliding window attention (SWA) [6, 3]. GQA significantly accelerates the inference speed, and also reduces the memory requirement during decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms collectively contribute to the enhanced performance and efficiency of Mistral 7B. Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference implementation1 facilitating easy deployment either locally or on cloud platforms such as AWS, GCP, or Azure using the vLLM [17] inference server and SkyPilot 2. Integration with Hugging Face 3 is also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat model fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B â Chat model. Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping large language models efficient. Through our work, our aim is to help the community create more affordable, efficient, and high-performing language models that can be used in a wide range of real-world applications.\n",
      "--------------------------------------------------\n",
      "ID: TWpNeE1DNHdOamd5TlNNNA==\n",
      "Arxiv ID: 2310.06825\n",
      "Title: Mistral 7B\n",
      "Score: 0.77977043\n",
      "Content: When evaluated on reasoning, comprehension, and STEM reasoning (specifically MMLU), Mistral 7B mirrored performance that one might expect from a Llama 2 model with more than 3x its size. On the Knowledge benchmarks, Mistral 7Bâ s performance achieves a lower compression rate of 1.9x, which is likely due to its limited parameter count that restricts the amount of knowledge it can store. Evaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts. # Instruction Finetuning To evaluate the generalization capabilities of Mistral 7B, we fine-tuned it on instruction datasets publicly available on the Hugging Face repository. No proprietary data or training tricks were utilized: Mistral 7B â Instruct model is a simple and preliminary demonstration that the base model can easily be fine-tuned to achieve good performance. In Table 3, we observe that the resulting model, Mistral 7B â Instruct, exhibits superior perfor- mance compared to all 7B models on MT-Bench, and is comparable to 13B â Chat models. An independent human evaluation was conducted on https://llmboxing.com/leaderboard. Model Chatbot Arena ELO Rating MT Bench WizardLM 13B v1.2 Mistral 7B Instruct Llama 2 13B Chat Vicuna 13B Llama 2 7B Chat Vicuna 7B Alpaca 13B 1047 1031 1012 1041 985 997 914 7.2 6.84 +/- 0.07 6.65 6.57 6.27 6.17 4.53 Table 3: Comparison of Chat models. Mistral 7B â Instruct outperforms all 7B models on MT-Bench, and is comparable to 13B â\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.models import (\n",
    "    VectorizedQuery\n",
    ")\n",
    "\n",
    "def generate_query_embedding(query):\n",
    "    # Set the embedding model\n",
    "    embed_model = \"mistral-embed\"\n",
    "\n",
    "    # Generate embedding for the query\n",
    "    embed = mistral.embeddings(model=embed_model, input=[query])\n",
    "    return embed.data[0].embedding\n",
    "\n",
    "# Pure Vector Search\n",
    "query = \"can you tell me about mistral LLM?\"\n",
    "\n",
    "vector_query = VectorizedQuery(\n",
    "    vector=generate_query_embedding(query), k_nearest_neighbors=3, fields=\"embedding\"\n",
    ")\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=None,\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"id\", \"arxiv_id\", \"title\", \"content\"],\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"ID: {result['id']}\")\n",
    "    print(f\"Arxiv ID: {result['arxiv_id']}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Score: {result['@search.score']}\")\n",
    "    print(f\"Content: {result['content']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: TWpNeE1DNHdOamd5TlNNeA==\n",
      "Arxiv ID: 2310.06825\n",
      "Title: Mistral 7B\n",
      "Score: 0.03201844170689583\n",
      "Content: Mistral 7B outperforms the previous best 13B model (Llama 2, [26]) across all tested benchmarks, and surpasses the best 34B model (LLaMa 34B, [25]) in mathematics and code generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [20], without sacrificing performance on non-code related benchmarks. Mistral 7B leverages grouped-query attention (GQA) [1], and sliding window attention (SWA) [6, 3]. GQA significantly accelerates the inference speed, and also reduces the memory requirement during decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms collectively contribute to the enhanced performance and efficiency of Mistral 7B. Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference implementation1 facilitating easy deployment either locally or on cloud platforms such as AWS, GCP, or Azure using the vLLM [17] inference server and SkyPilot 2. Integration with Hugging Face 3 is also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat model fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B â Chat model. Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping large language models efficient. Through our work, our aim is to help the community create more affordable, efficient, and high-performing language models that can be used in a wide range of real-world applications.\n",
      "--------------------------------------------------\n",
      "ID: TWpNeE1DNHdOamd5TlNNNA==\n",
      "Arxiv ID: 2310.06825\n",
      "Title: Mistral 7B\n",
      "Score: 0.031054403632879257\n",
      "Content: When evaluated on reasoning, comprehension, and STEM reasoning (specifically MMLU), Mistral 7B mirrored performance that one might expect from a Llama 2 model with more than 3x its size. On the Knowledge benchmarks, Mistral 7Bâ s performance achieves a lower compression rate of 1.9x, which is likely due to its limited parameter count that restricts the amount of knowledge it can store. Evaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts. # Instruction Finetuning To evaluate the generalization capabilities of Mistral 7B, we fine-tuned it on instruction datasets publicly available on the Hugging Face repository. No proprietary data or training tricks were utilized: Mistral 7B â Instruct model is a simple and preliminary demonstration that the base model can easily be fine-tuned to achieve good performance. In Table 3, we observe that the resulting model, Mistral 7B â Instruct, exhibits superior perfor- mance compared to all 7B models on MT-Bench, and is comparable to 13B â Chat models. An independent human evaluation was conducted on https://llmboxing.com/leaderboard. Model Chatbot Arena ELO Rating MT Bench WizardLM 13B v1.2 Mistral 7B Instruct Llama 2 13B Chat Vicuna 13B Llama 2 7B Chat Vicuna 7B Alpaca 13B 1047 1031 1012 1041 985 997 914 7.2 6.84 +/- 0.07 6.65 6.57 6.27 6.17 4.53 Table 3: Comparison of Chat models. Mistral 7B â Instruct outperforms all 7B models on MT-Bench, and is comparable to 13B â\n",
      "--------------------------------------------------\n",
      "ID: TWpRd01TNHdOREE0T0NNNQ==\n",
      "Arxiv ID: 2401.04088\n",
      "Title: Mixtral of Experts\n",
      "Score: 0.021171171218156815\n",
      "Content: Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â = Mistral Â° 20 â e LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params Figure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math. Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks. Size and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories. Note that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_query_embedding(query):\n",
    "    # Set the embedding model\n",
    "    embed_model = \"mistral-embed\"\n",
    "\n",
    "    # Generate embedding for the query\n",
    "    embed = mistral.embeddings(model=embed_model, input=[query])\n",
    "    return embed.data[0].embedding\n",
    "\n",
    "# Pure Vector Search\n",
    "query = \"can you tell me about mistral LLM?\"\n",
    "\n",
    "vector_query = VectorizedQuery(\n",
    "    vector=generate_query_embedding(query), k_nearest_neighbors=3, fields=\"embedding\"\n",
    ")\n",
    "\n",
    "results = search_client.search(\n",
    "    search_text=query,\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"id\", \"arxiv_id\", \"title\", \"content\"],\n",
    "    top=3 # Limit the number of results\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"ID: {result['id']}\")\n",
    "    print(f\"Arxiv ID: {result['arxiv_id']}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Score: {result['@search.score']}\")\n",
    "    print(f\"Content: {result['content']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground retrieved results from Azure AI Search to Mistral-Large LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral AI is a company based in Paris, France, developing large language models (LLMs). Their primary product is a large language model that is designed to understand and generate human-like text based on the input it receives. This model has been\n"
     ]
    }
   ],
   "source": [
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "query = \"can you tell me about mistral LLM?\"\n",
    "\n",
    "# Initialize the client\n",
    "client = MistralClient(api_key=mistral_api_key)\n",
    "\n",
    "# Convert the results to a string format\n",
    "context = \"\\n---\\n\".join([f\"ID: {result['id']}\\nArxiv ID: {result['arxiv_id']}\\nTitle: {result['title']}\\nScore: {result['@search.score']}\\nContent: {result['content']}\" for result in results])\n",
    "\n",
    "# Create the system message\n",
    "system_message = (\n",
    "    \"You are a helpful assistant that answers questions about AI using the \"\n",
    "    \"context provided below.\\n\\n\"\n",
    "    \"CONTEXT:\\n\"\n",
    "    + context\n",
    ")\n",
    "\n",
    "# Create the user message\n",
    "user_message = \"can you tell me about mistral LLM?\"\n",
    "\n",
    "# Create the messages parameter\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\",\n",
    "        content=system_message\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=\"user\",\n",
    "        content=user_message\n",
    "    )\n",
    "]\n",
    "\n",
    "# Generate the response\n",
    "chat_response = client.chat(\n",
    "    model=\"mistral-large-latest\",\n",
    "    messages=messages,\n",
    "    max_tokens=50,\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Results to Mistral-Large hosted in Azure AI Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get API key from left navbar in Mistral console\n",
    "azure_ai_studio_mistral_base_url = os.getenv(\"AZURE_AI_STUDIO_MISTRAL_BASE_URL\") or getpass.getpass(\"Enter your Azure Mistral Deployed Endpoint Base URL: \") # Example: https://<endpoint-name>.<region>.inference.ai.azure.com\"\n",
    "azure_ai_studio_mistral_api_key = os.getenv(\"AZURE_AI_STUDIO_MISTRAL_API_KEY\") or getpass.getpass(\"Enter your Azure Mistral API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral AI is a cutting-edge company based in Paris, France, developing large language models (LLMs). Their mission is to build safe, robust, and beneficial AI systems that can understand and generate natural language text.\n",
      "\n",
      "Mistral\n"
     ]
    }
   ],
   "source": [
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "# Initialize the client\n",
    "client = MistralClient(\n",
    "    endpoint=azure_ai_studio_mistral_base_url, \n",
    "    api_key=azure_ai_studio_mistral_api_key\n",
    ")\n",
    "\n",
    "# Convert the results to a string format\n",
    "context = \"\\n---\\n\".join([f\"ID: {result['id']}\\nArxiv ID: {result['arxiv_id']}\\nTitle: {result['title']}\\nScore: {result['@search.score']}\\nContent: {result['content']}\" for result in results])\n",
    "\n",
    "# Create the system message\n",
    "system_message = (\n",
    "    \"You are a helpful assistant that answers questions about AI using the \"\n",
    "    \"context provided below.\\n\\n\"\n",
    "    \"CONTEXT:\\n\"\n",
    "    + context\n",
    ")\n",
    "\n",
    "# Create the user message\n",
    "user_message = \"can you tell me about mistral LLM?\"\n",
    "\n",
    "# Create the messages parameter\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\",\n",
    "        content=system_message\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=\"user\",\n",
    "        content=user_message\n",
    "    )\n",
    "]\n",
    "\n",
    "# Generate the response\n",
    "chat_response = client.chat(\n",
    "    model=\"azureai\",\n",
    "    messages=messages,\n",
    "    max_tokens=50,\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
