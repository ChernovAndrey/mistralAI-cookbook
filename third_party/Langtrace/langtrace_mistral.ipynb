{"cells":[{"cell_type":"markdown","metadata":{"id":"g7j3YX8-WvMX"},"source":["# Setting up Langtrace with Mistral\n","\n","This Notebook shows the instructions for setting up OpenTelemetry based tracing for Mistral with Langtrace AI."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":4081,"status":"ok","timestamp":1731615462519,"user":{"displayName":"Karthik Kalyanaraman","userId":"06165623146440115541"},"user_tz":480},"id":"8HpEYe58UiOl","outputId":"232de61c-8294-403e-b031-a76a1f894612"},"outputs":[],"source":["%pip install mistralai langtrace-python-sdk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RPYBLKHLU3NW"},"outputs":[],"source":["\n","import os\n","\n","# Get your Langtrace api key from https://langtrace.ai\n","os.environ[\"LANGTRACE_API_KEY\"] = \"<YOUR_LANGTRACE_API_KEY>\"\n","\n","# Your Mistral key\n","os.environ[\"MISTRAL_API_KEY\"] = \"<YOUR_MISTRAL_API_KEY>\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LAcp_amwVL4S"},"outputs":[],"source":["from mistralai import Mistral\n","from langtrace_python_sdk import with_langtrace_root_span, langtrace\n","\n","# Initialize Langtrace\n","langtrace.init()\n","\n","# Decorate with root span to group spans together (https://docs.langtrace.ai/tracing/group_traces)\n","@with_langtrace_root_span(\"chat_complete\")\n","def chat_complete():\n","    model = \"mistral-large-latest\"\n","    client = Mistral()\n","    chat_response = client.chat.complete(\n","        model=model,\n","        messages=[\n","            {\n","                \"role\": \"user\",\n","                \"content\": \"I need 10 cocktail recipes with tequila other than the classics like margarita, tequila\"\n","            },\n","        ]\n","    )\n","    print(chat_response.choices[0].message.content)\n","\n","\n","if __name__ == \"__main__\":\n","    chat_complete()\n"]},{"cell_type":"markdown","metadata":{"id":"vp05Kt2YVgbv"},"source":["That's it! Now you should be able to see the traces for all your inference calls on Langtrace!\n","\n","![Trace 1](./mistral-langtrace-1.png)\n","\n","![Trace 2](./mistral-langtrace-2.png)\n","\n","![Trace 3](./mistral-langtrace-3.png)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPFTAJHx2doU1tWgCnfqECl","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
