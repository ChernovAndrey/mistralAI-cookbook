{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "789577ac-4396-484f-a2ca-776bb1a128a8",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"chainlit logo\" src=\"public/logo_light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "    </p>\n",
    "</center>\n",
    "\n",
    "The goal of this cookbook is to show how one can build a **Chainlit** application on top of **Mistral AI**'s APIs!\n",
    "\n",
    "We will highlight the reasoning capabilities of Mistral's LLMs by letting a self-reflective agent assess whether it has gathered enough information to answer _nested_ user questions, such as **\"What is the weather in Napoleon's hometown?\"**\n",
    "\n",
    "To answer such questions, our application should go through multiple-step reasoning: first get Napoleon's hometown, then fetch the weather for that location.\n",
    "\n",
    "You can read through this notebook or simply go with `chainlit run app.py` since the whole code is in `app.py`. \n",
    "You will find here a split of the whole application code with explanations:\n",
    "\n",
    "- [Requirements](#requirements)\n",
    "- [Define available tools](#define-tools)\n",
    "- [Agent logic](#agent-logic)\n",
    "- [On message callback](#on-message)\n",
    "- [Starter questions](#starter-questions)\n",
    "\n",
    "Here's a visual of what we will have built in a few minutes:\n",
    "\n",
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"chat visual\" src=\"public/chat-visual.jpg\" width=\"600\"/>\n",
    "        <br>\n",
    "    </p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e42fd5-ec5d-4038-b2d2-f6cfa1762819",
   "metadata": {},
   "source": [
    "<a id=\"requirements\"></a>\n",
    "## Requirements\n",
    "\n",
    "We will install `mistralai`, `chainlit` and `python-dotenv`. \n",
    "\n",
    "Be sure to create a `.env` file with the line `MISTRAL_API_KEY=` followed by your Mistral AI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a12c5cbe-ac48-498b-ae7e-1e81bee20fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mistralai\n",
      "  Using cached mistralai-0.4.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting chainlit\n",
      "  Downloading chainlit-1.1.305-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting httpx<1,>=0.25 (from mistralai)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting orjson<3.11,>=3.9.10 (from mistralai)\n",
      "  Using cached orjson-3.10.5-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (49 kB)\n",
      "Collecting pydantic<3,>=2.5.2 (from mistralai)\n",
      "  Using cached pydantic-2.7.4-py3-none-any.whl.metadata (109 kB)\n",
      "Collecting aiofiles<24.0.0,>=23.1.0 (from chainlit)\n",
      "  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting asyncer<0.0.3,>=0.0.2 (from chainlit)\n",
      "  Using cached asyncer-0.0.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting click<9.0.0,>=8.1.3 (from chainlit)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting dataclasses_json<0.6.0,>=0.5.7 (from chainlit)\n",
      "  Using cached dataclasses_json-0.5.14-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting fastapi<0.111.0,>=0.110.1 (from chainlit)\n",
      "  Using cached fastapi-0.110.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from chainlit)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting lazify<0.5.0,>=0.4.0 (from chainlit)\n",
      "  Using cached Lazify-0.4.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting literalai==0.0.607 (from chainlit)\n",
      "  Using cached literalai-0.0.607-py3-none-any.whl\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.6 in /Users/desaxce/miniconda3/envs/temp/lib/python3.12/site-packages (from chainlit) (1.6.0)\n",
      "Collecting numpy<2.0,>=1.26 (from chainlit)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: packaging<24.0,>=23.1 in /Users/desaxce/miniconda3/envs/temp/lib/python3.12/site-packages (from chainlit) (23.2)\n",
      "Collecting pyjwt<3.0.0,>=2.8.0 (from chainlit)\n",
      "  Using cached PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting python-multipart<0.0.10,>=0.0.9 (from chainlit)\n",
      "  Using cached python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting python-socketio<6.0.0,>=5.11.0 (from chainlit)\n",
      "  Using cached python_socketio-5.11.3-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from chainlit)\n",
      "  Using cached starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting syncer<3.0.0,>=2.0.3 (from chainlit)\n",
      "  Using cached syncer-2.0.3-py2.py3-none-any.whl\n",
      "Collecting tomli<3.0.0,>=2.0.1 (from chainlit)\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting uptrace<2.0.0,>=1.22.0 (from chainlit)\n",
      "  Using cached uptrace-1.24.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting uvicorn<0.26.0,>=0.25.0 (from chainlit)\n",
      "  Using cached uvicorn-0.25.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting watchfiles<0.21.0,>=0.20.0 (from chainlit)\n",
      "  Using cached watchfiles-0.20.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting chevron>=0.14.0 (from literalai==0.0.607->chainlit)\n",
      "  Using cached chevron-0.14.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting anyio<4.0.0,>=3.4.0 (from asyncer<0.0.3,>=0.0.2->chainlit)\n",
      "  Using cached anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses_json<0.6.0,>=0.5.7->chainlit)\n",
      "  Using cached marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses_json<0.6.0,>=0.5.7->chainlit)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/desaxce/miniconda3/envs/temp/lib/python3.12/site-packages (from fastapi<0.111.0,>=0.110.1->chainlit) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/desaxce/miniconda3/envs/temp/lib/python3.12/site-packages (from httpx<1,>=0.25->mistralai) (2024.6.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.25->mistralai)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in /Users/desaxce/miniconda3/envs/temp/lib/python3.12/site-packages (from httpx<1,>=0.25->mistralai) (3.7)\n",
      "Requirement already satisfied: sniffio in /Users/desaxce/miniconda3/envs/temp/lib/python3.12/site-packages (from httpx<1,>=0.25->mistralai) (1.3.0)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.25->mistralai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=2.5.2->mistralai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.4 (from pydantic<3,>=2.5.2->mistralai)\n",
      "  Using cached pydantic_core-2.18.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Collecting bidict>=0.21.0 (from python-socketio<6.0.0,>=5.11.0->chainlit)\n",
      "  Using cached bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting python-engineio>=4.8.0 (from python-socketio<6.0.0,>=5.11.0->chainlit)\n",
      "  Using cached python_engineio-4.9.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-api~=1.24 (from uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Using cached opentelemetry_api-1.25.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp~=1.24 (from uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Using cached opentelemetry_exporter_otlp-1.25.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation~=0.45b0 (from uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Using cached opentelemetry_instrumentation-0.46b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-sdk~=1.24 (from uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Using cached opentelemetry_sdk-1.25.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting importlib-metadata<=7.1,>=6.0 (from opentelemetry-api~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Using cached importlib_metadata-7.1.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc==1.25.0 (from opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http==1.25.0 (from opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_http-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc==1.25.0->opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Using cached googleapis_common_protos-1.63.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting grpcio<2.0.0,>=1.0.0 (from opentelemetry-exporter-otlp-proto-grpc==1.25.0->opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Using cached grpcio-1.64.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.25.0 (from opentelemetry-exporter-otlp-proto-grpc==1.25.0->opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.25.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting opentelemetry-proto==1.25.0 (from opentelemetry-exporter-otlp-proto-grpc==1.25.0->opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Using cached opentelemetry_proto-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests~=2.7 in /Users/desaxce/miniconda3/envs/temp/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http==1.25.0->opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit) (2.32.2)\n",
      "Collecting protobuf<5.0,>=3.19 (from opentelemetry-proto==1.25.0->opentelemetry-exporter-otlp-proto-grpc==1.25.0->opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools>=16.0 in /Users/desaxce/miniconda3/envs/temp/lib/python3.12/site-packages (from opentelemetry-instrumentation~=0.45b0->uptrace<2.0.0,>=1.22.0->chainlit) (69.5.1)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation~=0.45b0->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Using cached wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.46b0 (from opentelemetry-sdk~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Using cached opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting simple-websocket>=0.10.0 (from python-engineio>=4.8.0->python-socketio<6.0.0,>=5.11.0->chainlit)\n",
      "  Using cached simple_websocket-1.0.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses_json<0.6.0,>=0.5.7->chainlit)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<=7.1,>=6.0->opentelemetry-api~=1.24->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Using cached zipp-3.19.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting wsproto (from simple-websocket>=0.10.0->python-engineio>=4.8.0->python-socketio<6.0.0,>=5.11.0->chainlit)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/desaxce/miniconda3/envs/temp/lib/python3.12/site-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.25.0->opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/desaxce/miniconda3/envs/temp/lib/python3.12/site-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.25.0->opentelemetry-exporter-otlp~=1.24->uptrace<2.0.0,>=1.22.0->chainlit) (2.2.2)\n",
      "Using cached mistralai-0.4.1-py3-none-any.whl (19 kB)\n",
      "Downloading chainlit-1.1.305-py3-none-any.whl (4.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached asyncer-0.0.2-py3-none-any.whl (8.3 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
      "Using cached fastapi-0.110.3-py3-none-any.whl (91 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached Lazify-0.4.0-py2.py3-none-any.whl (3.1 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Using cached orjson-3.10.5-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (258 kB)\n",
      "Using cached pydantic-2.7.4-py3-none-any.whl (409 kB)\n",
      "Using cached pydantic_core-2.18.4-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Using cached python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Using cached python_socketio-5.11.3-py3-none-any.whl (76 kB)\n",
      "Using cached starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Using cached uptrace-1.24.0-py3-none-any.whl (8.6 kB)\n",
      "Using cached uvicorn-0.25.0-py3-none-any.whl (60 kB)\n",
      "Using cached watchfiles-0.20.0-cp37-abi3-macosx_11_0_arm64.whl (407 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "Using cached bidict-0.23.1-py3-none-any.whl (32 kB)\n",
      "Using cached chevron-0.14.0-py3-none-any.whl (11 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "Using cached opentelemetry_api-1.25.0-py3-none-any.whl (59 kB)\n",
      "Using cached opentelemetry_exporter_otlp-1.25.0-py3-none-any.whl (7.0 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.25.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_http-1.25.0-py3-none-any.whl (16 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.25.0-py3-none-any.whl (17 kB)\n",
      "Using cached opentelemetry_proto-1.25.0-py3-none-any.whl (52 kB)\n",
      "Using cached opentelemetry_instrumentation-0.46b0-py3-none-any.whl (29 kB)\n",
      "Using cached opentelemetry_sdk-1.25.0-py3-none-any.whl (107 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl (130 kB)\n",
      "Using cached python_engineio-4.9.1-py3-none-any.whl (57 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached importlib_metadata-7.1.0-py3-none-any.whl (24 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached simple_websocket-1.0.0-py3-none-any.whl (13 kB)\n",
      "Using cached wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached googleapis_common_protos-1.63.2-py2.py3-none-any.whl (220 kB)\n",
      "Using cached grpcio-1.64.1-cp312-cp312-macosx_10_9_universal2.whl (10.3 MB)\n",
      "Using cached zipp-3.19.2-py3-none-any.whl (9.0 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Using cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Installing collected packages: syncer, lazify, filetype, chevron, zipp, wrapt, tomli, python-multipart, python-dotenv, pyjwt, pydantic-core, protobuf, orjson, numpy, mypy-extensions, marshmallow, h11, grpcio, click, bidict, anyio, annotated-types, aiofiles, wsproto, watchfiles, uvicorn, typing-inspect, starlette, pydantic, opentelemetry-proto, importlib-metadata, httpcore, googleapis-common-protos, deprecated, asyncer, simple-websocket, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, httpx, fastapi, dataclasses_json, python-engineio, opentelemetry-semantic-conventions, opentelemetry-instrumentation, mistralai, literalai, python-socketio, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-exporter-otlp, uptrace, chainlit\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.2.0\n",
      "    Uninstalling anyio-4.2.0:\n",
      "      Successfully uninstalled anyio-4.2.0\n",
      "Successfully installed aiofiles-23.2.1 annotated-types-0.7.0 anyio-3.7.1 asyncer-0.0.2 bidict-0.23.1 chainlit-1.1.305 chevron-0.14.0 click-8.1.7 dataclasses_json-0.5.14 deprecated-1.2.14 fastapi-0.110.3 filetype-1.2.0 googleapis-common-protos-1.63.2 grpcio-1.64.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 importlib-metadata-7.1.0 lazify-0.4.0 literalai-0.0.607 marshmallow-3.21.3 mistralai-0.4.1 mypy-extensions-1.0.0 numpy-1.26.4 opentelemetry-api-1.25.0 opentelemetry-exporter-otlp-1.25.0 opentelemetry-exporter-otlp-proto-common-1.25.0 opentelemetry-exporter-otlp-proto-grpc-1.25.0 opentelemetry-exporter-otlp-proto-http-1.25.0 opentelemetry-instrumentation-0.46b0 opentelemetry-proto-1.25.0 opentelemetry-sdk-1.25.0 opentelemetry-semantic-conventions-0.46b0 orjson-3.10.5 protobuf-4.25.3 pydantic-2.7.4 pydantic-core-2.18.4 pyjwt-2.8.0 python-dotenv-1.0.1 python-engineio-4.9.1 python-multipart-0.0.9 python-socketio-5.11.3 simple-websocket-1.0.0 starlette-0.37.2 syncer-2.0.3 tomli-2.0.1 typing-inspect-0.9.0 uptrace-1.24.0 uvicorn-0.25.0 watchfiles-0.20.0 wrapt-1.16.0 wsproto-1.2.0 zipp-3.19.2\n"
     ]
    }
   ],
   "source": [
    "!pip install mistralai chainlit python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f4eff-e67a-4241-954b-04c29ba2dc45",
   "metadata": {},
   "source": [
    "<a id=\"define-tools\"></a>\n",
    "## Define available tools\n",
    "\n",
    "In the next cell, we define the tools, and their JSON definitions, which we will provide to the agent. We have two tools:\n",
    "- `get_current_weather` -> takes in a location\n",
    "- `get_home_town` -> takes in a person's name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fa631f5-0ef0-4c75-91ff-34e4a8a4204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "async def get_current_weather(location):\n",
    "    # Make an actual API call! To open-meteo.com for instance.\n",
    "    return json.dumps({\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"29\",\n",
    "        \"unit\": \"celsius\",\n",
    "        \"forecast\": [\"sunny\"],\n",
    "    })\n",
    "\n",
    "async def get_home_town(person: str) -> str:\n",
    "    \"\"\"Get the hometown of a person\"\"\"\n",
    "    return \"Ajaccio, Corsica\"\n",
    "\n",
    "\"\"\"\n",
    "JSON tool definitions provided to the LLM.\n",
    "\"\"\"\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_home_town\",\n",
    "            \"description\": \"Get the home town of a specific person\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"person\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name of a person (first and last names) to identify.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"person\"]\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "# This helper function runs multiple tool calls in parallel, asynchronously.\n",
    "async def run_multiple(tool_calls):\n",
    "    \"\"\"\n",
    "    Execute multiple tool calls asynchronously.\n",
    "    \"\"\"\n",
    "    available_tools = {\n",
    "        \"get_current_weather\": get_current_weather,\n",
    "        \"get_home_town\": get_home_town\n",
    "    }\n",
    "\n",
    "    async def run_single(tool_call):\n",
    "        function_name = tool_call.function.name\n",
    "        function_to_call = available_tools[function_name]\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "        function_response = await function_to_call(**function_args)\n",
    "        return {\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "            \"role\": \"tool\",\n",
    "            \"name\": function_name,\n",
    "            \"content\": function_response,\n",
    "        }\n",
    "\n",
    "    # Run tool calls in parallel.\n",
    "    tool_results = await asyncio.gather(\n",
    "        *(run_single(tool_call) for tool_call in tool_calls)\n",
    "    )\n",
    "    return tool_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9968ae44-e024-47d2-9b96-ac732921f67e",
   "metadata": {},
   "source": [
    "<a id=\"agent-logic\"></a>\n",
    "## Agent logic\n",
    "\n",
    "For the agent logic, we simply repeat the following pattern (max. 5 times):\n",
    "- ask the user question to Mistral, making both tools available\n",
    "- execute tools if Mistral asks for it, otherwise return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b34cce66-9c2b-4b98-9981-5207928b1764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from mistralai.client import MistralClient\n",
    "\n",
    "mai_client = MistralClient(api_key=os.environ[\"MISTRAL_API_KEY\"])\n",
    "\n",
    "async def run_agent(user_query: str):\n",
    "    messages = [{\n",
    "        \"role\": \"user\", \n",
    "        \"content\": user_query\n",
    "    }]\n",
    "\n",
    "    number_iterations = 0\n",
    "    answer_message_content = None\n",
    "\n",
    "    while number_iterations < 5:\n",
    "        completion = mai_client.chat(\n",
    "            model=\"mistral-large-latest\",\n",
    "            messages=messages,\n",
    "            tool_choice=\"auto\", # use `any` to force a tool call\n",
    "            tools=tools,\n",
    "        )\n",
    "        message = completion.choices[0].message\n",
    "        messages.append(message)\n",
    "        answer_message_content = message.content\n",
    "\n",
    "        if not message.tool_calls:\n",
    "            # The LLM deemed no tool calls necessary,\n",
    "            # we break out of the loop and display the returned message\n",
    "            break\n",
    "\n",
    "        tool_results = await run_multiple(message.tool_calls)\n",
    "        messages.extend(tool_results)\n",
    "\n",
    "        number_iterations += 1\n",
    "\n",
    "    await cl.Message(content=answer_message_content).send()\n",
    "    return answer_message_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28daf2a9-22d7-48a0-abf9-f45ba16c3896",
   "metadata": {},
   "source": [
    "<a id=\"on-message\"></a>\n",
    "## On message callback\n",
    "\n",
    "The callback below, properly annotated with `@cl.on_message`, ensures our `run_agent` function is called upon every new user message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c9960b4-d7cd-4d27-85fb-4bac26a21635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainlit as cl\n",
    "\n",
    "@cl.on_message\n",
    "async def main(message: cl.Message):\n",
    "    \"\"\"\n",
    "    Main message handler for incoming user messages.\n",
    "    \"\"\"\n",
    "    await run_agent(message.content)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "226ad995-3dd8-4205-b118-c44d200d0908",
   "metadata": {},
   "source": [
    "<a id=\"starter-questions\"></a>\n",
    "## Starter questions\n",
    "\n",
    "You can define starter questions for your users to easily try out your application, which will look like this:\n",
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"starters\" src=\"public/starters.jpg\" width=\"500\"/>\n",
    "        <br>\n",
    "    </p>\n",
    "</center>\n",
    "\n",
    "We have got many more Chainlit features in store (authentication, feedback, Slack/Discord integrations, etc.) to let you build custom LLM applications and really take advantage of Mistral's LLM capabilities.\n",
    "\n",
    "Please visit the <a href=\"https://docs.chainlit.io/\">Chainlit documentation</a> to learn more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "161fa63d-1465-436f-8076-280b7c70e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def set_starters():\n",
    "    return [\n",
    "        cl.Starter(\n",
    "            label=\"What's the weather in Napoleon's hometown\",\n",
    "            message=\"What's the weather in Napoleon's hometown?\",\n",
    "            icon=\"/images/idea.svg\",\n",
    "        ),\n",
    "        cl.Starter(\n",
    "            label=\"What's the weather in Paris, TX?\",\n",
    "            message=\"What's the weather in Paris, TX?\",\n",
    "            icon=\"/images/learn.svg\",\n",
    "        ),\n",
    "        cl.Starter(\n",
    "            label=\"What's the weather in Michel-Angelo's hometown?\",\n",
    "            message=\"What's the weather in Michel-Angelo's hometown?\",\n",
    "            icon=\"/images/write.svg\",\n",
    "        ),\n",
    "    ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
